# VirES server containers


## Prerequisites

### Install podman

To run the container-based VirES development server make sure `podman`
is installed on your computer.


### Creating instance secrets

Before starting the VirES pod/containers random instance secrets must be
generated by the provided script.  Namely, the secrets for the PosgreSQL
databases, Django session keys, and OAuth app client credentials.

```
./generate_secrets.sh
```

Once created, these secrets do not need be modified.

### Access to image repository

The VirES images are stored in a private repository which require
access credentials. These credentials are not included in this repository.

It is not necessary to have access to the repository to be able to build
and use the container images. 

If you need the access credentials for the container repository contact
the VirES team.


## Quick start

Step 1, pull or build the container images.

Step 2, pun
```
./start_pod.sh
```
and go to `http://localhost:8200` in your browser. Please note that it takes
some time to initialize the containers. Do not panic if the service shows
initially `503 Service Unavailable` it may not be initialized yet.
Give it a minute and try it again.

See below how to configure data location and register data.
How to get the data is not part of this guide.

## Container image hierarchy

 - `centos7` - Base CentOS 7 image
  - `database` - PosgreSQL DB server
  - `ingress` - Apache web server
  - `django-base` - common base Django image
    - `oauth-base` - base OAuth server
      - `oauth` - development OAuth server
    - `swarm-base` - base Swarm server image
      - `swarm` - development Swarm server image

Each of the container images has it is own directory with the container definitions.


## Understanding the container life-cycle

The VirES development setup uses a podman pod which binds a couple of separate
containers into a pod. Namely:

- database - PosgreSQL DB server container
- ingress - Web server publishing the static content and serving as a reverse proxy to the VirES services
- swarm - VirES Swarm application server
- ouath - VirES OAuth application server

Once created, the individual containers exist until removed manually.
These containers can be re-started and once running commands can be executed
in these containers.

In particular, they are not automatically replaces when newer built images appear.
To do so, the running containers must be stopped and removed first.
Then the newly created containers will use the up-to-date container images

This can be achieved by the provided helper scripts, e.g.,

```
./cnt swarm reload ; ./cnt swarm logs -f
```
(Note that the reload does not show any output. To see the container
initialization it is recommended to show the output of the podman logs)

## Container volumes

The containers use a mixture of named volumes and volumes mounded from
the host file-system (see the `./voluemes` folder).

The named volumes can be listed by (check for `vires-` prefix)
```
podman volume ls
```

See also other `podman volume` commands
```
podman pod ps --help
```

## Pod management

To start the pod run the `start_pod.sh` script witch creates and start the pod
named `vires-server-dev` and bootstraps the required containers

```
./start_pod.sh
```

To check that the pod was created successfully try, e.g.,

```
podman pod ps
```

See also other `podman pod` commands
```
podman pod ps --help
```


## PosgreSQL database initialization and management

The database container is started without any database created. The database
and the associated roles need to be generated from the credentials.

```
./cnt database exec -i create_db < ./volumes/secrets/oauth.conf
./cnt database exec -i create_db < ./volumes/secrets/swarm.conf
```

The `start_pod.sh` command takes care of the databases but
it needs executed manually to force an update of the credentials.

In addition, a few helper commands are provide to manage the instance databases:

```
./cnt database exec list_dbs
./cnt database exec list_db_users
./cnt database exec drop_db <db-name>
./cnt database exec drop_db_user <db-user>
```


## Django instances

The `swarm` and `oauth` instances are initialized automatically upon container
creation do not require manual intervention.

Instance data are updated at each container creation (reload of the container)
```
./cnt swarm reload
```

To force update the of the Django persistent instance configuration
(e.g., settings.py) remove the instance and restart the container
```
rm -fR ./volumes/swarm/{manage.py,swarm} ; ./cnt swarm reload
```

Since the VirES server components are mounted from their local development
folders any changes are visible immediately in the container, though,
in order to take an effect the running daemons must be restarted.
This is achieved by restarting of the containers.
```
./cnt swarm restart
```

For a running container, the `manage.py` scripts can be executed as
```
./cnt swarm exec runuser -u vires manage.py
```

## Web client installation

The web client is installed from the `contrib/WebClient-Framework.tar.gz`
upon the reload of the `swarm` container.

## Data registration

To register products in batch run the following command
```
./cnt swarm exec register_products
```

This command scans for data products in the configured data directory
and registers them to the server.

Beside the batch registration you can control the registered data
by the `manage.py` command
```
./cnt swarm exec manage.py --help
```

### Data location

By default the `swarm` container mounts data from the `../../data` directory
(the same default location as of the Vagrant VM).

To change the default location, set `VIRES_DATA` environment variable and
reload the `swarm` container.

```
export VIRES_DATA=<your custom data directory>
./cnt swarm reload ; ./cnt swarm logs -f
```

## Building new containers

Before you start a new container build make sure you update the common
container tag in `tag.conf` file

```
IMAGE_TAG="20230725T1531"
```

Make the tag is set to the time-stamp produced by the following command
```
date -u +"%Y%m%dT%H%M"
```

The run the build scripts for the individual containers
```
./build.sh <name>
```

Or build all containers in one shot (skips existing containers).
```
./build.sh
```
